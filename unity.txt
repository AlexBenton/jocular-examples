12  
Working with Unity Pro 
In this chapter we cover
   Creating an application for the Rift using Unity Pro
   Creating a scene for the Rift
   Importing and using the Oculus Unity 4 Pro Integration package 
   Using player data from the Oculus Calibration tool
   Adding a Rift-friendly skybox
   Using head tracking in your control scheme
   Creating a stand-alone build for the Rift
Unity Pro is a game development IDE and cross-platform 3D graphics engine developed by Unity Technologies.  With the Oculus Unity 4 Pro Integration package - a collection of prefabs, scripts, and art assets provided by Oculus VR - you have a great tool for quickly building Rift compatible software. 
Downloads and requirements
Unity Pro 4.1 or later is required when working with the Oculus integration package.  If you are new to Unity, we highly recommend working through the Editor tutorial  on the Unity web site. This tutorial will get you up to speed with the Unity interface and Unity development concepts.
Also required is the Oculus integration package from the Oculus VR, which can be obtained from the Oculus VR Developer Center Downloads page:
Page: https://developer.oculusvr.com/?action=dl
File: “Unity 4 Pro Integration”
This is the only package you need from Oculus for this chapter. Unlike most of the previous chapters in this book, we won’t be making use of the Oculus SDK directly and you won’t even need to have it installed.
The example scenes and scripts used in this chapter are available from [github location]. Note that as Unity uses C# and Javascript, we’ll be using those languages for the example code in this chapter rather than C++ as used in previous chapters. 

In this chapter, we’ll show you how to use Unity Pro and the Oculus integration package to create a simple scene that can be used with the Rift.  The basic integration steps are:
1.  Create a scene.
2.  Import the Oculus integration package into your project’s assets.
3.  Use the Oculus character controller prefab (OVRPlayerController) or the Oculus camera prefab (OVRCameraController) with a character controller in your scene.
We’ll be covering these steps in detail in the following sections.  Then, with the basics in place, we’ll take a look at rounding out your application for the Rift environment. This will include adding a Rift-friendly skybox (the standard Unity skybox implementation does not work with the Rift), using the Rift configuration tools, and using head tracking as part of your control scheme. We’ll also look at packaging up the application for a standalone build.
Working without a Rift 
Using Unity Pro and the Oculus Integration package is a great way to start developing for the Rift, even before you have one. When building your VR application, you should include both a non-Rift player controller (and be sure to include using the mouse to control where the character looks, in both x and y directions) and a Rift player controller. Then, by enabling and disabling the appropriate player controllers, you can easily test-run your application with and without the Rift attached. We’ll be pointing out best practices as we go, along with some problems that are only visible when viewed on the Rift, so with good planning you can avoid some unpleasant surprises.



Although our complete example scenes are available on github  , we recommend that you create your own similar scene to work with. We will be going over all of the steps needed to create each scene and by doing each step as we go, you’ll be sure to get a solid grip on using the Oculus integration package.  Start by downloading the OVR Unity 4Pro Integration add-on from developer.oculusvr.com.  It’s not the smallest of downloads, because it includes all the assets you’ll need to build the “Tuscany” demo and Rift scenes of your own. While that download is in progress, you can start creating a scene of your own.
12.1    Creating a scene for the Rift
You add Rift support to a Unity project from within a scene, so to get started you’ll need a scene to work with.  If you just want to see the Rift in action, for a bare minimum scene all you need is a plane to move around on, a light to see by, and some simple objects to give you something to look at.  
An important caveat about creating scenes for the Rift, even for a bare minimum scene, is that virtual size matters!
12.1.1  Use real-life scale for Rift scenes
By default, the units in Unity are meters, and one Unity unit represents one meter in space .  When creating a scene for the Rift, it is best to design the scene to scale, and there are two excellent reasons for doing so. First, all components in the Oculus integration package were created to scale and it will save you time and customization to already have a world where these components fit.  For example, the default character controller in the Oculus integration package is 1.85 meters tall and you can’t just stick a 1.85 meter tall character into a 10 centimeter tall world.  And, while character size may seem easily adjusted, it is not the only adjustment you will need to make. You would also need to update the stereo camera parameters for your one-tenth scale world to look correct. Second, VR worlds designed to scale are more comfortable for players to use and can help curb motion sickness, which is an important issue to consider when creating VR.
Creating comfortable virtual environments
Designing to scale isn’t the only thing you can do to reduce curb motion sickness. Using darker textures, avoiding objects that flicker (including skinny objects that can flicker unintentionally), and avoiding crazy patterns are all things that can help. For a more complete description of what you can do, see chapter 8. 



Now that we know what the bare minimum scene requirements are, let’s build a scene to work with. 
12.1.2  Creating an example scene
For the scenes used in this chapter, we didn’t want to be someone who just does the bare minimum. So, for each of the scenes in our example project, we used the Terrain Assets standard Unity package to add some flair and to create a more pleasant scene  to view - a sand beach with palm trees (figure 12.1).  

 
Figure 12.1: Our sample game scene 
To create a similar scene, create a new project and open a new scene.  When creating the new project, be sure to include the Terrain Assets standard unity package. You should also include the Character Controller assets, as we will be using those in a later section. To create a scene like the one in figure 12.1, first add a plane for the character to stand on. Use a 10 x 10 plane positioned at the origin point (GameObject > Create Other > Plane, Transform - Scale  - 10x1x10) to give yourself some room to roam around on and rename this object Beach.  Next, add a Directional Light set above the scene, so you can see where you are going (GameObject > Create Other > Directional Light).  You can now dress the scene up a bit by dragging the “GoodDirt” texture from the Terrain Assets (Project:Assets > Standard Assets > Terrain Assets > Terrain Textures) onto the plane to give it the appearance of sand and add some trees by dragging a few “Palm” assets (Project:Assets > Standard Assets > Terrain Assets > Trees > Palm) into the scene to have some objects to look at.  When you are done, you should have a game scene that looks something like figure 12.x.
With the scene ready, the next step is to import the Oculus integration package into the project.
12.2    Importing the Oculus Unity integration package 
If you have not already done so, download the OVR Unity 4Pro Integration zip file from developer.oculusvr.com.  Extract the contents of the zip file to a stable location; these are the Oculus resources you’ll be using in Unity.  While you’re there, the download also includes a handy Integration Guide that’s worth a read.
To import the Oculus Unity integration package into your project, complete the following steps:
1.  From the top menu, under Assets choose Import Package -> Custom Package.
2.  Find the OculusUnityIntegration folder that you extracted and select OculusUnityIntegration.unitypackage. 
Alternatively, with Unity already running, you can navigate on your desktop to where you’ve unzipped the integration package and double-click OculusUnityIntegration.unitypackage directly.
You will now see a list of the items that will be imported into your project (figure 12.2). For a complete description of these packages, see the OculusUnityIntegrationGuide.pdf that came with the OVR Unity download.
 
Figure 12.2: The OVR packages to import into your project.
Click Import to add the packages into your project.
After importing the Oculus integration package, you should see an OVR directory and a Plugins directory added to your project’s assets (figure 12.3).
 
Figure 12.3:  The OVR assets and plugins added to your project.
After you have saved your scene, you should also see an Oculus menu item added to your Unity main menu (figure 12.4).

 
Figure 12.4: The Oculus menu
With the Oculus integration package in place, you can see that it includes two prefabs: OVRCameraController and OVRPlayerController. The OVRCameraController prefab is a stereo camera that is used in place of a single Unity camera, and the OVRPlayerController prefab is an OVRCameraController prefab attached to a character controller. These two prefabs are your fast-pass tickets to getting a Unity application running on the Rift. 
12.3    Using the OVRPlayerController prefab
The OVRPlayerController prefab is the easiest way to get a scene running on the Rift as it contains both a character controller and the Rift cameras. Let’s take a closer look at what makes up the OVRPlayerController prefab, before we put it into a scene.
12.3.1  The OVRPlayerController prefab components
As shown in the OVRPlayer prefab hierarchy (figure 12.5), the OVRPlayerController prefab consists of the OVRPlayerController object and its two child objects : ForwardDirection and the OVRCameraController prefab. 

 
Figure 12.5: The OVRPlayerController prefab hierarchy
The OverPlayerController object itself is a CharacterController with three C# scripts attached (figure 12.6) that handle character movement and create a simple menu system. 
 
Figure 12.6: The OverPlayerController inspector panel
We can see in the Inspector that the default value for the CharacterController Height is 2 and the Radius is 0.5. This means that the character will take up a capsule-shaped space that is two meters high and 1 meter in diameter.  When designing your VR world, keep this size in mind so that you can be sure to give your character enough space to comfortably move around without unintentionally colliding into other objects. 
The three C# scripts attached to the OVRPlayerController prefab are OVRPlayerController.cs, OVRGamepadController.cs, and OVRMainMenu.cs. These three scripts can be found in the Project menu in OVR >Scripts.
The OVRMainMenu.cs script controls the loading of different scenes.  It also provides a menu to configure various Rift settings, such as the player eye height and IPD (inter-pupillary distance) and whether or not the Rift’s prediction feature is enabled.  (Recall that prediction is used to reduce the impact of latency on your scene.  For a complete description of prediction, see Using Prediction in chapter 3.)  It also allows for storing these settings for later.  The OVRMainMenu.cs script can safely be removed from OVRPlayerController and added to another GameObject.
The OVRPlayerController.cs and OVRGamepadController.cs scripts allow you to navigate the scene using the A, S, D, W keys and mouse controls, or a connected game controller.   A complete description of the control layout and supported game controllers can be found in the OculusUnityIntegrationGuide.pdf. 
We can now turn our attention to the two child objects: ForwardDirection and the OVRCameraController prefab (figure 12.7).

 
Figure 12.7: The OVRPlayerController prefab hierarchy
The ForwardDirection GameObject contains only the matrix which determines motor control direction.  This game object is also a convenient location to place the body geometry seen by the player.
The OVRCameraController prefab is the eyes of the Rift integration package. It includes a stereo camera (CameraLeft and CameraRight) and its attached scripts handle the image processing for proper display on the Rift. This prefab can be used independently of the OVRPlayerController and we will cover its use, structure, and attached scripts in a later section. 
Now that we know what the OVRPlayerController prefab is, let’s see how it works.
12.3.2  Adding the OVRPlayerController prefab to your scene
As the OvrPlayerController contains both a stereo camera and a character controller, all you need to do to have functioning Rift scene, is add it to the scene. In our examples, Beach_OvrPlayerController is the completed scene using this prefab. 
To add the OVRPlayerController prefab to your scene:
1.  Drag the OVRPlayerController prefab on to the Scene view.

The OVRPlayerController prefab can be found under the Project menu, in Assets > OVR >Prefabs. 
2.  Reposition the OVRPlayerController to a good place in the scene. 

It should be located above the plane and not colliding with any objects. The OVRPlayerController prefab character controller has a default height of 2 units and radius of 0.5 units. As the position is the center of the capsule, you should set the Transform Position Y to 1 so that the OVRPlayerController is positioned above the beach.   


3.  Remove or disable any other Audio Listeners you might have in the scene. 

You will get an error if there is more than one audio listener in the scene. Both the default camera and the OVRCameraController prefab contain audio listeners.
If you switch the scene to the Game view, you will see that the scene now shows left and right views (figure 12.8).

 
Figure 12.8: Left and right views as seen in the Game view
You can give this scene a test run. If in extended desktop mode, drag the 'Game' window onto the Rift screen and click the small square next to the 'x' on the top-right; or if in cloned (mirrored) mode, select Maximize on Play and click the play icon.  If the Rift is connected and active, the scene will be displayed with left and right images in the familiar distortion ovals figure 12.9)
. 
Figure 12.9: The scene displayed in the Rift’s oval views. 
If the Rift is not connected and active, you will see the left and right views simply as rectangles (as in the previous game view snapshot.) This is because the scripts used to process the images for proper display on the Rift retrieve required distortion values from the Rift headset itself. 
Sidebar: Headache warning! “Maximize on play” is not full screen
When testing your game in the Unity Editor, using “Maximize on play” will maximize the game view to 100% of your Editor window, which is not quite full screen. Because the Editor window borders are on the game window, you’ll still lose a small percentage of the screen to desktop elements. This reduction means the alignment is slightly off and it also means you aren’t seeing the actual scale.  Both of these side effects can lead to major (real) headaches. While using “Maximize on play” is a very quick way to iterate, for a better and more comfortable testing environment, do a stand-alone build for each test run.



Looking down, you will see that you are just a floating head in space - you don’t have a body.   This can be disorientating for many users and you can help curb motion sickness in your applications by providing your character with a body. To add a body, you can use the ForwardDirection game object attached to the OVRController prefab to hold the body geometry.  (See Chapter 8 for more ideas on how to prevent motion sickness.)
You can navigate the scene using the A, S, W, D keys and mouse controls, or a connected game controller.  You can also access the default OVRPlayerController menu by pressing the spacebar (figure 12.10), which displays the Rift’s current settings (Eye Height, IPD, etc).

 
Figure 12.10: The OVRPlayerController menu
When you look at this menu through the Rift, you’ll notice that the closer the text is to the center of the view, the easier it is to read. When designing your own menus, you will want to place them as close to the center of the player’s view as possible.  Anything that forces users to move their eyes in their sockets rather than to move their head to look at something can cause significant eye strain. In addition, the closer you get to the edge of the screen, the more the distortion effects the view. 
If you find that these controls do not meet the needs of your application, you can use the OVRCameraController prefab with the Unity standard controller or with your own custom controller. 
12.4    Using the OVRCameraController prefab 
The OVRCameraController prefab serves as the eyes of the Oculus Integration package. It provides a stereo camera and image processing scripts for proper display to the Rift.  When integrating the OVRCameraController into your own controller, you will need to make adjustments to the default values to create a comfortable experience for your users.  To understand what kind of adjustments you might need to make and how to make those adjustment, we need to understand how this prefab and its attached scripts work.
12.4.1  The OVRCameraController prefab components
The OVRCameraController prefab consists of the OVRCameraController object and two child objects, CameraLeft and CameraRight (figure 12.11).
 
Figure 12.11: The OVRCameraController hierarchy
Let’s start by looking at the OVRCameraController object and its attached scripts (figure 12.12).
 
Figure 12.12: OVRCameraController expanded in the Inspector
As you can see in figure 12.12, OVRCameraController has two C# scripts attached, OVRCameraController and OVRDevice. Like the scripts for the OVRPlayerController, they can be found in the Project menu in OVR > Scripts.
The OVRDevice script is the main interface between Unity and the Rift hardware and contains wrappers for all exported SDK functions. This script lets you specify if the tracker should reset when the scene loads and lets you adjust the prediction delta used (figure 12.13).  Prediction is used to reduce the apparent effects of latency in your scene - for a complete description of prediction, see Using Prediction in chapter 3.  Because it allows you to change the Rift prediction and reset values, this script should only be declared once.
 
Figure 12.13: The OVRDevice Script values 
The OVRCameraController script is the interface between Unity and the cameras and it is where all camera control should be done.  You can see the public values in this script by expanding the Inspector (figure 12.14). Full descriptions of the public values can be found in the OculusUnityIntegrationGuide.pdf.

 
Figure 12.14: The OVRCameraController public values expanded in the Inspector
There are a couple of gotchas to be aware of when working with this script.  First, Tracker Rotates Y is used by OVRPlayerController to allow head tracking to influence the forward direction of the controller. If Follow Orientation is set to the parent transform of this component and Tracker Rotates Y is set to true, you will get a feedback loop in orientation.  


Second, although you can lower latency by setting Call in PreRender, it may not work on all systems.  When Call in PreRender is set, the tracker is read in the OnPreRender stage of the rendering pipeline, rather than in the OnPreCull stage.  OnPreRender comes later in the pipeline than OnPreCull, which means that the tracker is read closer to the actual rendering of the image, resulting in lower latency for the user; but, it also means that the culling stage misses out on precise and accurate knowledge of the head orientation.  That means that some culling optimizations, and other optimizations built into Unity’s pipeline between the two methods, could function incorrectly.  For this reason, the Call in PreRender feature should be used with caution and hands-on testing; see the Known Issues section of the OculusUnityIntegrationGuide.pdf  for more details.
There are two child objects attached to OVRCameraController: CameraLeft and CameraRight (figure 12.15). As you might surmise, these are both Unity camera objects and CameraLeft provides the view for the left eye and CameraRight provides the view for the right eye.  Both camera objects have the OVRCamera script and the OVRLensCorrection script attached. CameraRight also has an Audio Listener attached.  The OVRCamera script can be found in the Project menu in OVR > Scripts while the OVRLensCorrection script is found in the Project menu in OVR > OVRImageEffects.

 
Figure 12.15: CameraLeft and CameraRight expanded in the Inspector.
The OVRCamera script renders into a Unity camera class.  This script handles getting the input from the Rift head tracker and the positioning and rotation of the camera.  As you can see in figure 12.15, the Depth is set to 0 for CameraRight and 1 for CameraLeft. If you look at the function SetCameraOrientation in the OVRCamerascript, you’ll see that the script only updates the orientation of the camera if the camera depth is equal to 0. This means that only CameraRight receives the orientation data and calculates the proper transform to apply to child objects. If you want to use the camera transform, for example to attach a menu, you need to parent the object to CameraRight. If you parent the object to CameraLeft, you are likely to see noticeable latency on the right eye. 
The OVRCamera  script is also responsible for handling the final camera output. So although only CameraRight is used to update the orientation, both left and right cameras require this script. 
The OVRLensCorrection script is used to add full screen lens correction on the camera. This correction includes correction for the lens distortion and lens position. For more information about the correction needed, see How the Rift display is different: Why it matters to you in Chapter 4 and for how distortion is implemented, see chapter 5.
The distortion correction is a post-processing effect and must be the last post-processing effect applied. If you add additional post-processing effects, you can change the order of a component by clicking the gear on the far right, then choosing move up or move down. 
Now that we have a handle on what the OVRCameraController prefab is, let’s take a look at it in action. 
12.4.2  Using the OVRCameraController prefab in your scene
To add the OVRCameraController prefab to your scene you need to attach it to a moving object, such as a character controller.  For our example, we will:
1.  Add a character controller to the scene using Unity standard assets.
2.  Add the OVRCameraController prefab to the character controller.
3.  Make changes for the Unity standard asset scripts  so that they will work with OVRCameraController.
4.  Set the default player eye height.
The next sections will walk you through these steps.
ADD A CHARACTER CONTROLLER
In our example scene, Beach_OvrCameraController,  we used a character controller capsule with the FPSInputController.js and MouseLook.cs scripts found in the standard Unity Character Controller assets package. To add the same character controller to your scene, complete the following steps:
1.  Add an empty gameObject to your scene and rename it Player to make it easier to keep track of it. 

Under the Game Object menu, select Create Empty. 
2.  Add a Character Controller to the Player gameObject and adjust the Transform Position of the Player object so that it is located above the Beach plane and not colliding with any objects.

Under the Component menu, select Physics > Character Controller. The default Height for the Character Controller is 2, so, for the Transform of the Player object, set Y Position to 1 to reposition the Player object so that it is not colliding with the beach.

3.  Add the FPSInputController.js and MouseLook.cs control scripts to your Player capsule.

You can find these scripts in the Project menu under Standard Assets > Character Controllers > Sources > Scripts. Simply drag the scripts you want to use on to your Player (figure 12.16).  If you don’t have theses assets, to add the standard Unity character controller assets to your project, select Assets > Import Package > Character Controller and click Import in the items to import window that appears. 
 
Figure 12.16: The Player character controller expanded in the Inspector.
You now have a character controller in your scene (figure 12.17). If you want to test this setup with a single camera, add a camera to the player as a child object and do a test run, but, be sure to remove the single camera before adding the OVRCameraController prefab.
ADD THE OVRCAMERACONTROLLER PREFAB TO THE CHARACTER CONTROLLER
With the character controller and scripts in place, you can now add the Rift stereo cameras to the Player. 
Grab the OVRCameraController prefab and add it as a child of your Player as seen in figure 12.17. You can find the OVRCameraController prefab under the Project menu, in Assets > OVR > Prefabs.

 
Figure 12.17: The OverCameraController added as a child of the Player in the scene hierarchy 
You should disable or delete any other character controllers and cameras in the scene, such as the OVRPlayerController we used in the last section, or the main default camera if you are using a new scene.  Go ahead and give the scene a test run. 

 
Figure 12.18: The scene displayed in the Rift’s oval views.
If a Rift is connected and active, the scene will be displayed with left and right images in the Rift’s oval views (figure 12.18).  If a Rift is not connected and active, you will see the left and right views simply as rectangles. 


You should be able to move the character around but you’ll notice that mouse look does not work. For that to function, we need to make a few adjustments so that the script works with the OVRCameraController.
MAKE CHANGES TO THE SCRIPTS SO THAT THEY WILL WORK WITH OVRCAMERACONTROLLER
The scripts need to know about the OVRCameraController to work properly.  Make the following changes:
1.  In the Inspector for OVRCameraController, set Follow Orientation to OVRCameraController (figure 12.19).
 
Figure 12.19: The inspector for OVRCameraController
2.  Make the following edits to FPSInputController.js so that the script recognizes the OVRCameraController:
    At the top, add a new public variable called ovrCamera of type GameObject:
   public var ovrCamera : GameObject;
   Change this line: 
motor.inputMoveDirection = transform.rotation * directionVector;

 to this, so that the transform used is from the object stored in ovrCamera: 

motor.inputMoveDirection = ovrCamera.transform.rotation * directionVector;  

3.  Go into the Inspector for the Player and for First Person Controller, set the value of ovrCamera to CameraRight (figure 12.20). Now the transform used is that of CameraRight.  

 
Figure 12.20: The Inspector for the Player and for First Person Controller
    Make sure to use CameraRight and not CameraLeft. The Rift orientation data only goes to cameraRight.

4.  In the Player Inspector for the MouseLook script (figure 12.21), set the Axes for your testing environment. If you are testing with a Rift attached, we recommend setting Axes to MouseX as you will be using the Rift to look in all directions. If testing without a RIft, we recommend setting it to MouseXAndY so that you can use the mouse to look in all directions. 
 
Figure 12.21: the MouseLook script in the Player Inspector
5.  Give it a test run. 


You should now be able to navigate the scene and use mouse look.  
If you tested both the OVRPlayerController and the Unity Standard Player controller, there are two things that you might have noticed.  First, the default player speeds for OVRPlayerController are much slower than those in the Standard Unity Player controller.  Character speed plays a big part in how comfortable the VR environment you are creating is and in general, you want to use real-world speed and abilities (for more information, see chapter 8).  Second, the height of your point of view probably felt more natural when using OVRPlayerController.
SET THE DEFAULT PLAYER EYE HEIGHT 
The player eye height is set by the OVRCameraController script attached to the OVRCameraController.  Let’s look at the OVRCameraController script in the Inspector and focus on the values for Camera Root Position and Neck Position (figure 12.22).
 
Figure 12.22: OVRCameraController script in the Inspector and focusing on the values for Camera Root Position and Neck Position.
As you can see, the default Y values for Camera Root Position and Neck Position are set to 0.  The default Eye Center Position is set to [0, 0.15, 0.09], reflecting the typical offset of human eyes from the base of the skull; we use the offset from the base of the skull because that is the pivot about which the head--and the Rift--will be rotated by the player.  
If you used a 2 unit high capsule for your controller, your character’s eye position would look something like figure 12.23.

 
Figure 12.23: The default eye position for a 2-unit high character controller
The result is that the character’s view is just 0.15 meters off the ground - not a very natural point of view for a human. You should set the default eye height to something that would be reasonably comfortable for most people to use by adjusting the Camera Root Position and Neck Position.  We recommend that you do not change the Eye Center Position. The Eye Center Position is relative to the Neck Position, and increasing the Eye Center Position Y value can result in the disorienting sensation of bending over forwards or backwards when you are simply looking up or down.
For example, the OVRPlayerController prefab uses a 2 unit capsule and sets the Y value for Camera Root Position at 1, and the Y value for Neck Position at 0.7, as seen in figure 12.24, resulting in a player eye height of 1.85 meters.  
 

Figure 12.24: The eye height as set in the OVRPlayerController prefab
Of course, not all humans are 1.85 meters tall. In fact, most of them aren’t  so think about your audience and adjust the values to something that fits your average user. 
While you can (and should!) set the default player eye height, a better option is to use the user’s actual eye height which, if set, can be obtained from the Oculus Calibration Tool. 
12.5    Setting up and using the IPD and player eye height: Working with the Oculus calibration tool
Included in the Integration package is the Oculus Calibration tool. This tool allows users to create a personal profile for their height and their inter-pupillary distance (IPD). Setting the player height to the user’s actual height and setting the distance between the virtual stereo cameras to the user’s actual IPD can help curb motion sickness for your users. You should encourage the user to set these values and then, of course, you should make use of them when they do.  The tool also calibrates against the local ambient magnetic field, which will improve the accuracy of the Rift’s magnetometer and prediction capabilities.
As we mentioned when covering the OVRCameraController prefab components, the player eye height and IPD values are set by the attached OVRCameraController.cs script  (figure 12.25). 
 

Figure 12.25: OVRCameraController script in the Inspector and focusing on the values for IPD and Use Player Eye Height.
For IPD, the default is set to the average human IPD value (0.064) or if the IPD has been set in the Oculus Configuration Tool, it will use the value found there.  You can override the default value by using the SetIPD()method in OVRCameraController.cs .
You can also choose to use the player eye height value from the Oculus Configuration tool simply by checking Use Player Eye Height (by default it is not enabled). If you choose to use the player eye height, the Neck Position will be adjusted in relation to the Camera Root Position and Eye Center Position so that the total of all three equals the Player Eye Height.
One caveat about using the player height: while this is enabled, writing to the Neck Position using the SetNeckPosition() method in OVRCameraController.cs won’t change anything as this method only updates the neck position if UsePlayerEyeHeight != true. 
Having a character of the appropriate height is one step towards rounding out the application. Let’s take another and add a skybox to our scene. 
12.6    Adding a skybox
Skyboxes are simply very large cubes drawn behind all other graphics in a scene. They are a common way of placing a virtual scene inside a detailed, perhaps photorealistic, horizon which will always be beyond the user’s reach.
To add a skybox in Unity, you would typically choose Edit > Render Settings from the menu bar and drag a Skybox Material to the Skybox Material slot in the Inspector.  The Unity engine would then paint a background cubemap behind the camera.  However, the Rift uses stereo cameras and it requires the images displayed be corrected for the lenses used. Unfortunately, with the standard Unity skybox implementation, the custom projection matrix is not taken into account when the skybox is drawn. If you try to use the standard Unity skybox, it will look fine in the game scene on your monitor, but if you view the scene on the Rift, the sky appears doubled (figure 12.26). 

 
Figure 12.26: The standard Unity skybox implementation as viewed on the monitor and on the Rift
To add a skybox to your Rift application, you need to use a custom skybox. A custom skybox is just a large texture-mapped cube centered on the player.  By placing the skybox in the scene, rather than using Unity’s built-in skybox mechanism, the custom projection matrix will be used when the skybox is rendered.   
Sidebar: Shortcut to creating a custom skybox
If you are looking for a shortcut to creating a mapped object for the skybox that will allow you to use Unity skybox materials (your own, or one of the skybox materials from the Unity Standard Skyboxes package, which you can import from Assets > Import Package > Skyboxes), a couple of Rift developers have put out scripts that can be used to create a skybox cube mesh. For our example files, we used the script found at http://stereoarts.jp/SkyboxMesh.cs.  To use this script, add an empty game object to your scene and attach the script to that game object. In the inspector for the script, drag a skybox material on to the skybox material slot. 



Remember, if you are porting a scene to the Rift, and you used the standard Unity Skybox implementation, be sure to remove that skybox from your scene for use on the Rift. 
12.7    Using Rift head tracking as part of your control scheme 
The Oculus integration scripts and prefabs take care of using the Rift head tracker data to change the point of view for you, but the Rift head tracker data can be used in other ways as well. If you think of the head tracker data as one more way to get input from your user, you can take your application in more immersive and interesting directions.  For example, the “Trial of the Rift Drifter”  game by Aldin Dynamics uses the head tracker data to allow the user to shake their head “yes” or “no” to answer in-game questions. Other games use it to allow you to gaze at an object to select it, or to use your gaze to set the cross-hair to aim a weapon. 
In our next example scene, Beach_movecrates (figure 12.27), we started with the scene used to demonstrate OVRCameraController prefab and then added several cubes to the scene to give us some objects to play with.  Let’s call these cubes “crates” - every game needs crates. When you gaze at a crate, it turns blue. If you keep it in your sight for 2 seconds, it turns red and then you are able to move the crate to another point on the beach simply by turning your head to look at where you want the crate to go. When the crate collides with the beach or with another crate, it turns white and stays put until you pick it up again.
 

Figure 12.27: Moving crates using input from the Rift.
We need to create two scripts for this example, one that handles selecting and moving the crates and one that handles the collision that puts the crate down. Unity uses C#, Boo, and Javascript and for this example, we used JavaScript. 
12.7.1  Selecting and moving the crates
We’ll start with the script that handles selecting and moving the crates, the movegaze.js script in listing 12.1, which we will attach to the CameraRight object.  


Listing 12.1 -  moveegaze.js 
#pragma strict

var cameraLeft : Camera;
var cameraRight : Camera;

var startTime = System.DateTime.UtcNow;
var attachedObject : GameObject = null;
var lookedatObject : GameObject = null;

function Awake() {                                                 #A
 var cameras;                                                      #A
 var ovrCameraController : GameObject;                             #A
 ovrCameraController = GameObject.Find("OVRCameraController");     #A
 cameras =     
    ovrCameraController.GetComponentsInChildren(Camera);           #A
 for (var camera : Camera in cameras){                             #A
   if(camera.name == "CameraLeft"){                                #A
     cameraLeft = camera;                                          #A
   }
   if(camera.name == "CameraRight"){                               #A
     cameraRight = camera;                                         #A
   }
 }
 if((cameraLeft == null) || (cameraRight == null)) {
   Debug.Log("WARNING: Unity Cameras in OVRCameraController not found!");
 }
}

function Update() {
 var camRightTransform : Transform = cameraRight.transform;
 var camLeftTransform : Transform = cameraLeft.transform; 
 var forward = camRightTransform.forward;                         #B
 var position = 
    (camRightTransform.position + camLeftTransform.position) / 2; #C
 if (attachedObject == null) { 
   if (Physics.Raycast (ray, hit, 100)) {                          #D
     if ((currentTime - startTime).TotalSeconds > 2) {
       attachedObject = lookedatObject;
       attachedObject.rigidbody.useGravity = false;           #E
       attachedObject.rigidbody.isKinematic = false;               #F
       attachedObject.transform.parent = cameraRight.transform;    #G
       attachedObject.renderer.material.color = Color.red;
     } else {
       lookedatObject = hit.collider.gameObject;
       lookedatObject.renderer.material.color = Color.blue;
     }
   } else {
     if (lookedatObject != null) {
       lookedatObject.renderer.material.color = Color.white;
     }
     startTime = currentTime;
     lookedatObject = null;
   }
 }
 if (attachedObject != null) {
   if (attachedObject.renderer.material.color = Color.white) {   #H
     startTime = currentTime;                                    #H
     attachedObject = null;                                      
   }
 }
}

#A Find the cameras called CameraLeft and CameraRight that are attached to OVRCameraController.
#B Forward is same vector for both cameras. So just pick one.
#C Set position to halfway between the two cameras.
#D By casting a ray that originates at the point halfway between the two cameras, we can find out what is in our line of sight.
#E To move the cube around without having it affected by gravity, set gravity to false.
#F To be able to detect a collision, set isKenamatic to false.
#G The orientation data from the Oculus Rift is only applied to cameraRight.  To move an object in the scene with the Rift, make them child objects of the cameraRight object. 
#H After the object has been put down, set startTime to the currentTime.

For this script to work, there are two things we need to do. First, we need to attach the script to an object in the scene and for that we’ll use CameraRight.  Second, for any objects that we don’t want to detect, we need to set them to ignore the raycast.
ATTACHING THE SCRIPT TO CAMERARIGHT
For this script to work properly, you must attach the script to an object in the scene.
 
Figure 12.x: CameraRight in the Inspector
In our examples we have attached the script to CameraRight (figure 12.x). 
SETTING THE BEACH TO IGNORE THE RAYCAST
The script changes the renderer color blue for any collider object the raycast hits. To prevent the beach from turning blue, in the inspector for the Beach, set the layer to Ignore Raycast (figure 12.28). 
 
Figure 12.28: Setting the Layer for the Beach in the Inspector
Now that we can pick the crates up, we need a script to put the crates down. 
12.7.2  Using collision to put the selected crate down
We are moving the crate using the input from the headset by attaching the crate to CameraRight.  When the crate is attached to the camera, it always stays at the center of your view - you can’t look away from it to put it down. We need to use some other mechanism to determine when to remove the crate from the parent camera, and, in our example, the mechanism used is a collision. 
To handle the collision, we need a script attached to the object we are moving. This script (listing 12.1) will detect the collision, remove the parent transform, and then reset the crate so that it can be selected again.
Listing 12.1 - cubecollision.js
#pragma strict

function OnCollisionEnter (col : Collision) {
renderer.material.color = Color.white;                      #A
transform.parent = null;                                    #B
  if(col.gameObject.name == "Beach") {
    transform.rotation = Quaternion.Euler(0, 0, 0);         #C
    transform.position.y = 
      col.gameObject.transform.position.y + transform.localScale.y / 2 
        + .1; #C 
    rigidbody.isKinematic = true;                          #D
    rigidbody.useGravity = true;                           #D
  }else {
    rigidbody.useGravity = true;                             #F
  } 
}

function OnCollisionStay() {
  rigidbody.AddForce(transform.forward * 20);                #G
}

#A Set the crate render color to white
#B Put the crate down by setting the transform parent to null so that the crate is no longer attached to the camera
#C If the crate collided with the beach, position the crate so that it is not intersecting the beach at any point.
#D Enable gravity, and, because we no longer want the crate to detect collisions, set isKinematic to true.
#E If the crate collided with another crate, only enable gravity. We want the crate to fall but we still want it to detect when it has collided with the beach so that it can be properly positioned and we want to detect if it still colliding with another crate.
#G If the crate lands in a position such that it is colliding with another crate, apply force until the crates move apart.

For the script to work, it needs to be attached to the crates and the crates need to be properly set up to work with the script. 
ATTACHING THE COLLISION SCRIPT AND SETTING UP THE CRATES
For the crates we are using cubes of various sizes and locations. To add a cube, select GameObject > Create Other > Cube. As you will want to add multiple cubes to the beach, we suggest you drag the cube created to the Asset window to make it a prefab and rename the prefab “Crate”.  Now you can drag as many crates as you want to the scene while only having to update the prefab to properly set up the crates. When adding a crate to the scene, the  exact position of the crate is not important, however, it is important that it not be colliding with the beach.
To detect collisions between any two objects, at least one of the objects in the collision must have a rigidbody attached. We want the crates to be able to detect collisions with the beach and with other crates,  so we need to attach a rigidbody to the crate. Select the crate prefab and then select Component > Physics > Rigidbody to attach a rigidbody.
Having a rigidbody attached means the crates will act under the control of the physics engine, and its position will be influenced by gravity. We want the crates to fall, so for the rigidbody attached to the crate, check useGravity to set it to true.  Note that we have left isKinematic unchecked. This means the crate is under the control of the physics engine and if you placed the crate hovering in the air, it will fall to the beach when you run the scene. 
We also need to attach the cubecolllsion.js script to the crates.

 
Figure 12.29: The crate expanded in the Inspector
With the crates set up and the script attached (figure 12.29, it is time to give the scene a test run. You should be able to wander around the beach, look at crates and move them to another spot on the beach. 
With our example complete, let’s look at packaging up the application as a stand-alone build. 
12.8    Creating a standalone build 
As we mentioned in the sidebar after the first test run, using “Maximize on Play” in the Unity Editor, while quick and easy, isn’t a great way to test the application because it isn’t exactly full-screen. Because it is slightly less than full screen, the scale is slightly off and the alignment may also be slightly off. Combined, these two issues can make testing very uncomfortable.  For a better testing environment, you will want to do a standalone build. 
When creating a standalone build for the Rift application there are a couple of things you should to take into consideration, namely the default screen resolution and quality settings.
12.8.1  Default screen resolution
First, when building a stand-alone application that runs on the Rift, you should set the default screen resolution to the actual screen resolution of the Rift DK1 (1280 x 800). Setting the screen resolution raises the odds that you’ll default to what’s present.  To set the default screen resolution, select Edit > Project Settings > Player. 
 
Figure 12.30: Player Project settings for a DK1 Rift in the Inspector
In the Inspector for Player Settings under Resolution and Presentation, for a DK1 Rift, uncheck Default is full screen, and then set the Default Screen Width to 1280 and the Default Screen Height to 800 as seen in figure 12.30. 
12.8.2  Quality settings 
Next, you’ll want to look at the quality settings for your build, select Edit > Project Settings > Quality and look at the setting for Anti Aliasing (figure 12.31).

 
Figure 12.31: Quality Project setting in the Inspector
The Rift uses stereo rendering, which effectively reduces the horizontal resolution by 50%. You can enable or increase anti-aliasing to compensate for the reduced resolution. Start with a value of 2X multi-sampling and experiment to see what works best for your application.  Remember to consider your target platform; you may be targeting your game at systems less powerful than your own.
12.8.3  Targeting the display 
A word of warning, as of this writing, targeting the display in fullscreen mode is listed as a known issue that will be fixed in a future release of Unity. 
12.9    Summary
In this chapter we covered
   Creating Rift applications using Unity Pro and the Oculus Unity integration package
Rift development is a new and very exciting endeavor and Unity Pro with the Oculus Unity integration package provides a great way to quickly get software running on the Rift. While we have been able to point out a number of areas that require extra thought when creating virtual reality applications for the Rift, such as menu placement and motion sickness triggers, the truth is, what really makes an awesome Rift environment is still being discovered. We haven’t seen the definitive VR application yet. But, we hope to see it soon and we hope that one of you, our readers, will be the person who creates it. We can’t wait to see what you build!          

